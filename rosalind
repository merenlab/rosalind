#!/usr/bin/env python
# -*- coding: utf-8

import sys
import time
import glob
import random
import shutil
import platform
import argparse
import requests
import subprocess
import itertools

import json
import tempfile
from pathlib import Path
from datetime import datetime
from collections import defaultdict


DEFAULT_CONFIG_FILENAME = "config.yaml"


class SlurmMonitor:
    def __init__(self, args):
        self.args = args

        # user args
        A = lambda x: self.args.__dict__[x] if x in self.args.__dict__ else None
        self.webhook = None
        self.group_id = A('group_id')
        self.config = self._load_config(A('config_file'))
        self.slack_token = self.config.get("slack_token")
        raw_channel = self.config.get("slack_channel")
        self.slack_channel = raw_channel[1:] if isinstance(raw_channel, str) and raw_channel.startswith("#") else raw_channel

        self.check_interval = A('squeue_check_interval')
        self.user_jobs_interval = A('user_jobs_interval')
        self.overall_summary_interval = A('overall_summary_interval')
        self.overall_summary_at_start = A('overall_summary_at_start')

        self.stdout_only = A('stdout_only')
        self.testing = A('testing')

        self.cluster_name = self.config.get("cluster_name", "your cluster")
        self.bot_mention = self.config.get("bot_mention", "<@rosalind>")
        self.quiet_on_days = [day.lower() for day in self.config.get("quiet_on_days", [])]
        self.known_users = self.config.get("known_users", {})
        self.usage_log_path = Path(self.config.get("usage_log_path", Path(__file__).with_name("usage_metrics.json")))
        self.usage_retention_days = self.config.get("usage_retention_days", 7)

        # secrets (webhook/token) must come from config
        self.webhook = self.config.get("webhook")

        self.new_job_messages = [
            "There are new jobs on {cluster} for *{user}*! Buckle up! :rocket:",
            "Heads up, *{user}*! Fresh jobs just landed on {cluster}! :eyes:",
            "Hey *{user}*, your jobs just hit the queue! :construction_worker:",
            "New {cluster} jobs incoming for *{user}*! :computer:",
            "*{user}* has new jobs running on {cluster}. :chart_with_upwards_trend:",
            "Looks like *{user}* is keeping {cluster} busy again! :hourglass_flowing_sand:",
            "Incoming workload detected! *{user}* has jobs in the queue! :robot_face:",
            "New jobs for *{user}* on {cluster}. :gear:",
            "Brace yourselves, *{user}*'s jobs are running on {cluster}! :zap:",
            "Guess who's back? It's *{user}* with more jobs on {cluster}! :sunglasses:",
            "{cluster}'s got work to do! *{user}* just submitted new jobs! :fire:",
            "Hey *{user}*, your jobs are officially in action on {cluster}! :runner:",
            "New SLURM jobs detected! *{user}* is on a roll? :clipboard:",
            "Great news: *{user}* has new jobs in {cluster}'s pipeline! :brain:",
            "All systems go! *{user}* has jobs running on {cluster}! :rocket:",
            "New jobs alert! *{user}* is back at it on {cluster}! :alarm_clock:",
            "Time to put {cluster} to work! *{user}* just submitted some jobs! :computer:",
            "Processing request confirmed! *{user}*'s jobs are in the system! :white_check_mark:"
        ]
        self.finished_job_messages = [
            "Mission accomplished! All {cluster} jobs by *{user}* are done! :tada:",
            "No more jobs for *{user}*! {cluster} can finally take a breather. :relieved:",
            "*{user}*'s jobs have wrapped up! Time to celebrate! :champagne:",
            "That's a wrap! *{user}* has no more jobs on {cluster}! :clapper:",
            "All jobs done! *{user}* can rest easy now. :sleeping:",
            "{cluster} just finished running all jobs by *{user}*! Nice work! :muscle:",
            "Break time! *{user}*'s jobs are all completed. :coffee:",
            "All clear! *{user}* has no active jobs on {cluster}. :white_check_mark:",
            "Another batch bites the dust! *{user}* is all done. :boom:",
            "Processing complete! *{user}*'s jobs are no more! :robot_face:",
            "Compute cycle complete! *{user}* has finished all jobs! :star:",
            "All of *{user}*'s jobs have crossed the finish line! :checkered_flag:",
            "{cluster} says goodbye to *{user}*'s jobs. Until next time! :wave:",
            "No more crunching numbers for *{user}*! Jobs are done! :bar_chart:",
            "All of *{user}*'s computations are history! :scroll:",
            "Ding! *{user}*'s job queue is empty now! :bell:",
            "Nothing left in the {cluster} queue for *{user}*! :ghost:",
            "Job execution complete! *{user}*'s tasks are finished! :white_check_mark:",
            "Well done, *{user}*! All your {cluster} jobs are complete. :sports_medal:",
            "{cluster} is free from *{user}*'s jobs! :bird:"
        ]

        if self.testing:
            self.stdout_only = True

        # some default variables
        self.previous_users = set([])

        try:
            if not shutil.which("squeue") and not self.testing:
                raise RuntimeError("You don't have `squeue` on this computer to run this program without the `--testing` flag :/")
        except RuntimeError as e:
            print(f"\n\033[91mERROR:\033[0m {e}\n")
            sys.exit(1)

        try:
            if not self.webhook and not self.stdout_only:
                raise RuntimeError("Missing Slack webhook in config.yaml and `--stdout-only` not set. Either provide a "
                                   "webhook in your config or run with --stdout-only for local output.")
        except RuntimeError as e:
            print(f"\n\033[91mERROR:\033[0m {e}\n")
            sys.exit(1)


    def get_time(self, time_str, in_seconds=False):
        """Convert the ugly time format (DD-HH:MM:SS, HH:MM:SS, MM:SS, or SS) into either human readable format, OR into secondss."""

        # parse the time str properly
        if '-' in time_str:
            days, rest = time_str.split('-')
            days = int(days)
            hours, minutes, seconds = list(map(int, rest.split(':')))
        else:
            parts = list(map(int, time_str.split(':')))
            if len(parts) == 3:  # H:M:S format
                hours, minutes, seconds = parts
                days = 0
            elif len(parts) == 2:  # M:S format
                minutes, seconds = parts
                hours = 0
                days = 0
            elif len(parts) == 1:  # S format
                seconds = parts[0]
                minutes = 0
                hours = 0
                days = 0

        if in_seconds:
            return days * 86400 + hours * 3600 + minutes * 60 + seconds
        else:
            readable_time = []
            if days:
                readable_time.append(f"{days}d")
            if hours:
                readable_time.append(f"{hours}h")
            if minutes:
                readable_time.append(f"{minutes}m")
            if seconds:
                readable_time.append(f"{seconds}s")

            if readable_time:
                if days:
                    human_readable_time = f"about {(days * 24) + hours} hours"
                elif hours:
                    if hours > 1:
                        human_readable_time = f"about {hours} hours and {minutes} minutes"
                    else:
                        human_readable_time = f"about one hour and {minutes} minutes"
                elif minutes:
                    if minutes > 1:
                        human_readable_time = f"about {minutes} minutes"
                    else:
                        human_readable_time = "about one minute"
                else:
                    human_readable_time = "like a few seconds (lol)"

                return human_readable_time


    def get_random_slack_notification_for_jobs(self, user, event):
        """Generates a random SLURM notification message for Slack for new/finished jobs by a given user."""

        if event == "new":
            templates = self.new_job_messages
            default_template = "{user} has new jobs on {cluster}."
        elif event == "finished":
            templates = self.finished_job_messages
            default_template = "All jobs for {user} finished on {cluster}."
        else:
            raise ValueError("Invalid event type. Use 'new' or 'finished'.")

        template = random.choice(templates or [default_template])
        try:
            return template.format(user=user, cluster=self.cluster_name)
        except (KeyError, IndexError):
            return template


    def get_slurm_job_data(self, squeue_output=None, job_state="RUNNING"):
        """Fetch the current slurm queue status using squeue and parse it"""

        # define the columns we want from the squeue along with specific column widths
        columns_of_interest = "%11i %35j %20u %8C %13m %15T %15M %15l %12D %R"
        column_widths       = [11, 35, 20, 8, 13, 15, 15, 15, 12, None]
        column_names        = ["JOBID", "NAME", "USER", "CPUS", "MIN_MEMORY", "STATE", "TIME", "TIME_LIMIT", "NODES", "NODELIST"]

        # run the command and collect the output
        if not squeue_output:
            try:
                if self.group_id:
                    command = ["squeue", "-A", self.group_id, "-o", columns_of_interest]
                else:
                    command = ["squeue", "-o", columns_of_interest]

                response = subprocess.run(command, capture_output=True, text=True, check=True)
                squeue_output = response.stdout
            except subprocess.CalledProcessError as e:
                return f"Error fetching slurm queue: {e}"

        lines = squeue_output.strip().split("\n")

        # fill in the job data
        job_data = []
        for line in lines[1:]:
            parts = []
            start = 0

            for width in column_widths:
                if width is None:  # Last column extends to the end
                    parts.append(line[start:].strip())
                else:
                    parts.append(line[start:start + width].strip())
                    start += width + 1

            job = {name: (int(value) if name == "CPUS" else value) for name, value in zip(column_names, parts)}

            if job['STATE'] == job_state:
                job_data.append(job)

        return job_data


    def get_known_user_name(self, user, prefer_name=False):
        """Return friendly identifier for a user (optionally prefer human name over Slack mention)."""

        entry = self.known_users.get(user)
        if isinstance(entry, dict):
            if prefer_name:
                return entry.get("name") or entry.get("slack") or user
            return entry.get("slack") or entry.get("name") or user
        return entry if entry else user


    def get_current_users(self, job_data):
        """Extracts a set of unique users from the Slurm queue output."""

        if not len(job_data):
            return set([])

        users = set([self.get_known_user_name(j['USER']) for j in job_data])

        return users


    def check_user_changes(self, job_data=None):
        """Check for new and finished users and send Slack notifications."""

        if job_data == None:
            job_data = self.get_slurm_job_data()

        current_users = self.get_current_users(job_data)

        # find out hat has changed
        new_users = current_users - self.previous_users
        finished_users = self.previous_users - current_users

        for user in new_users:
            self.message(self.get_random_slack_notification_for_jobs(user, 'new'))

        for user in finished_users:
            self.message(self.get_random_slack_notification_for_jobs(user, 'finished'))

        self.previous_users = current_users


    def summarize_jobs(self, job_data=None):
        """Summarizes job statistics per user based on the provided job output."""

        if job_data == None:
            job_data = self.get_slurm_job_data()

        # first figure out the order of users based on their CPU usage (so we can order the summary
        # of all jobs based on CPU usage)
        user_cpu_usage = defaultdict(int)

        for job in job_data:
            user_cpu_usage[job['USER']] += job['CPUS']

        users_by_cpu_usage = [user_id for user_id, total_cpus in sorted(user_cpu_usage.items(), key=lambda x: x[1], reverse=True)]

        # next, generate another dict that combines all the jobs that belong to the same user
        user_jobs = {}

        for job in job_data:
            user_id = job['USER']
            if user_id not in user_jobs:
                user_jobs[user_id] = []

            user_jobs[user_id].append(job)

        # now it is time to build the summary
        summaries = []
        for user_id in users_by_cpu_usage:
            user_name = self.get_known_user_name(user_id)
            jobs = user_jobs[user_id]
            total_jobs = len(jobs)
            total_cpus = sum(job['CPUS'] for job in jobs)
            longest_job = max(jobs, key=lambda x: self.get_time(x['TIME'], in_seconds=True))
            most_recent_job = min(jobs, key=lambda x: x['TIME'])

            if total_jobs == 1:
                summary = (f"> *{user_name}* has a single job, _{longest_job['NAME']}_, that is using a total of *{total_cpus}* "
                           f"{'CPUs' if total_cpus > 1 else 'CPU'} and has been running for *{self.get_time(longest_job['TIME'])}*.")
            else:
                summary = (f"> *{user_name}* has *{total_jobs}* jobs using a total of *{total_cpus}* CPUs. "
                           f"The longest job, _{longest_job['NAME']}_ has been running for *{self.get_time(longest_job['TIME'])}* "
                           f"and their most recently started job is _{most_recent_job['NAME']}_.")

            summaries.append(summary)

        jobs_summary = '\n\n'.join(summaries)

        if len(jobs_summary):
            message = f"""A *status update* from {self.cluster_name} (by {self.bot_mention} on head node _{platform.node()}_ at *{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*)\n\n{jobs_summary}"""
            self.message(message)
            self._maybe_upload_usage_figure()


    def message(self, message):
        """Send a message to Slack using the webhook (or print it to the terminal)"""

        today = datetime.today()
        day_name = today.strftime("%A").lower()

        print('--- Slack Message ---')
        print(message)
        print()

        if self.webhook and not self.stdout_only and day_name not in self.quiet_on_days:
            payload = {"text": message}
            response = requests.post(self.webhook, json=payload)
            return response.status_code, response.text
        else:
            return None, None

    def log_usage(self, job_data):
        """Persist lightweight usage metrics for later visualization."""

        timestamp = datetime.utcnow().isoformat() + "Z"
        total_cpus = sum(job["CPUS"] for job in job_data) if job_data else 0
        total_jobs = len(job_data)
        total_nodes = self._count_unique_nodes(job_data)
        total_mem_mb = sum(self._parse_mem_to_mb(job.get("MIN_MEMORY", "")) for job in job_data)
        per_user = defaultdict(lambda: {"cpu": 0, "mem_mb": 0})
        for job in job_data:
            per_user[job["USER"]]["cpu"] += job["CPUS"]
            per_user[job["USER"]]["mem_mb"] += self._parse_mem_to_mb(job.get("MIN_MEMORY", ""))

        entry = {
            "timestamp": timestamp,
            "total_cpus": total_cpus,
            "total_jobs": total_jobs,
            "total_nodes": total_nodes,
            "total_mem_mb": total_mem_mb
        }

        # attach per-user snapshot
        entry["users"] = {k: {"cpu": v["cpu"], "mem_mb": v["mem_mb"]} for k, v in per_user.items()}

        existing = []
        try:
            if self.usage_log_path.is_file():
                existing = json.loads(self.usage_log_path.read_text())
        except Exception:
            existing = []

        cutoff = datetime.utcnow().timestamp() - (self.usage_retention_days * 86400)
        pruned = []
        for row in existing:
            try:
                ts = datetime.fromisoformat(row["timestamp"].replace("Z", "")).timestamp()
                if ts >= cutoff:
                    pruned.append(row)
            except Exception:
                continue

        pruned.append(entry)

        try:
            self.usage_log_path.parent.mkdir(parents=True, exist_ok=True)
            self.usage_log_path.write_text(json.dumps(pruned, indent=2))
        except Exception as e:
            print(f"Warning: could not write usage log: {e}")

    def _parse_mem_to_mb(self, mem_str):
        """Parse Slurm memory strings like 4000M, 4G into MB; best effort."""

        if not mem_str:
            return 0

        try:
            if mem_str.lower().endswith('g'):
                return float(mem_str[:-1]) * 1024
            if mem_str.lower().endswith('m'):
                return float(mem_str[:-1])
            if mem_str.lower().endswith('k'):
                return float(mem_str[:-1]) / 1024
            return float(mem_str) / (1024 * 1024)  # assume bytes
        except Exception:
            return 0

    def _count_unique_nodes(self, job_data):
        """Count unique nodes from NODELIST entries."""

        nodes = set()
        for job in job_data:
            nodelist = job.get("NODELIST", "")
            if not nodelist or nodelist in ("(null)", "None"):
                continue
            for token in nodelist.replace('+', ',').split(','):
                token = token.strip()
                if token:
                    nodes.add(token)
        return len(nodes)

    def _resolve_slack_channel(self, channel):
        """Best effort conversion of channel names to IDs using the Slack token, when available."""

        if not channel or not self.slack_token:
            return channel

        if isinstance(channel, str) and channel[:1] in ("C", "G", "D"):
            return channel  # already looks like an ID

        name = channel.lstrip("#")

        try:
            response = requests.get(
                "https://slack.com/api/conversations.list",
                headers={"Authorization": f"Bearer {self.slack_token}"},
                params={
                    "exclude_archived": True,
                    "limit": 1000,
                    "types": "public_channel,private_channel"
                }
            )
            if response.status_code != 200:
                print(f"Warning: could not resolve Slack channel '{channel}': {response.text}")
                return channel

            payload = response.json()
            if not payload.get("ok"):
                print(f"Warning: could not resolve Slack channel '{channel}': {payload.get('error')}")
                return channel

            for ch in payload.get("channels", []):
                if ch.get("name") == name or ch.get("name_normalized") == name:
                    return ch.get("id") or channel
        except Exception as e:
            print(f"Warning: exception while resolving Slack channel '{channel}': {e}")

        return channel

    def _ensure_slack_channel_membership(self, channel_id):
        """Attempt to join the target channel so uploads do not fail with not_in_channel."""

        if not self.slack_token or not channel_id:
            return False

        try:
            response = requests.post(
                "https://slack.com/api/conversations.join",
                headers={"Authorization": f"Bearer {self.slack_token}"},
                data={"channel": channel_id}
            )
            if response.status_code != 200:
                print(f"Warning: could not join Slack channel {channel_id}: {response.text}")
                return False

            payload = response.json()
            if payload.get("ok") or payload.get("error") == "already_in_channel":
                return True

            print(f"Warning: could not join Slack channel {channel_id}: {payload.get('error')}")
        except Exception as e:
            print(f"Warning: exception while joining Slack channel {channel_id}: {e}")

        return False

    def _upload_file_to_slack(self, channel_id, file_path, title=None):
        """Upload a file to Slack using the external upload flow (V2)."""

        if not channel_id or not self.slack_token:
            return False, "missing_token_or_channel"

        file_path = Path(file_path)
        file_name = file_path.name
        file_size = file_path.stat().st_size

        headers = {"Authorization": f"Bearer {self.slack_token}"}

        # Step 1: Get an upload URL from Slack
        try:
            response = requests.post(
                "https://slack.com/api/files.getUploadURLExternal",
                headers=headers,
                data={"filename": file_name, "length": file_size},
            )
            body = response.json() if response.status_code == 200 else {}

            if not body.get("ok"):
                return False, body.get("error", "failed_to_get_upload_url")

            upload_url = body["upload_url"]
            file_id = body["file_id"]
        except Exception as e:
            return False, f"get_upload_url_failed: {e}"

        # Step 2: Upload the file content to the provided URL
        try:
            with open(file_path, "rb") as fh:
                upload_response = requests.post(upload_url, files={"file": fh})

            if upload_response.status_code not in (200, 201):
                return False, f"file_upload_failed: {upload_response.status_code}"
        except Exception as e:
            return False, f"file_upload_failed: {e}"

        # Step 3: Complete the upload and share to the channel
        try:
            complete_payload = {
                "files": json.dumps([{"id": file_id, "title": title or file_name}]),
                "channel_id": channel_id,
            }
            complete_response = requests.post(
                "https://slack.com/api/files.completeUploadExternal",
                headers=headers,
                data=complete_payload,
            )
            complete_body = complete_response.json() if complete_response.status_code == 200 else {}

            if complete_body.get("ok"):
                return True, None

            return False, complete_body.get("error", "complete_upload_failed")
        except Exception as e:
            return False, f"complete_upload_failed: {e}"

    def _maybe_upload_usage_figure(self):
        """Generate usage figure and upload/save depending on configuration."""

        figure_output_path = self.usage_log_path.with_suffix(".png")
        figure_path = self._build_usage_figure(output_path=figure_output_path)
        if not figure_path:
            return

        if not self.slack_token or not self.slack_channel:
            print(f"Usage figure saved to {figure_path} (Slack upload not configured).")
            return

        upload_channel = self._resolve_slack_channel(self.slack_channel)

        # Even if this fails we still try the upload and surface the Slack error.
        self._ensure_slack_channel_membership(upload_channel)

        try:
            ok, error_msg = self._upload_file_to_slack(
                channel_id=upload_channel,
                file_path=figure_path,
                title=f"Usage for {self.cluster_name}",
            )
            if not ok:
                hint = ""
                if error_msg == "not_in_channel":
                    hint = " (invite the Slack app to the channel or grant it access)"
                elif error_msg == "channel_not_found":
                    hint = " (check the channel ID or name in config)"
                elif error_msg == "missing_scope":
                    hint = " (token needs files:write scope)"
                elif "get_upload_url" in str(error_msg):
                    hint = " (check that your token has files:write scope)"
                elif "complete_upload" in str(error_msg):
                    hint = " (file uploaded but sharing failed; check channel permissions)"
                print(f"Warning: failed to upload usage figure to Slack: {error_msg}{hint}")
        except Exception as e:
            print(f"Warning: exception during Slack upload: {e}")

    def _build_usage_figure(self, output_path=None):
        """Build usage plots for nodes/CPUs/memory over available data (max one week)."""

        try:
            import matplotlib.pyplot as plt
            import matplotlib.dates as mdates
        except ImportError:
            print("matplotlib not available; skipping usage figure.")
            return None

        if not self.usage_log_path.is_file():
            return None

        try:
            data = json.loads(self.usage_log_path.read_text())
        except Exception:
            return None

        now_ts = datetime.utcnow().timestamp()
        cutoff_ts = now_ts - (7 * 24 * 3600)
        points = []
        for row in data:
            try:
                ts = datetime.fromisoformat(row["timestamp"].replace("Z", "")).timestamp()
            except Exception:
                continue
            if ts >= cutoff_ts:
                points.append((datetime.fromtimestamp(ts), row))

        if not points:
            return None

        times = [p[0] for p in points]
        nodes = [p[1].get("total_nodes", 0) for p in points]
        user_snapshots = [p[1].get("users", {}) for p in points]

        # use a pleasant style if available
        try:
            plt.style.use("seaborn-v0_8-darkgrid")
        except Exception:
            plt.style.use("ggplot")

        all_users = sorted({user_id for snap in user_snapshots for user_id in snap})
        color_map = {
            user_id: plt.cm.tab10(i % 10) for i, user_id in enumerate(all_users)
        }

        def plot_user_stack(ax, metric_key, title, value_transform=None):
            user_totals = defaultdict(float)
            for snap in user_snapshots:
                for user_id, vals in snap.items():
                    value = vals.get(metric_key, 0)
                    if value_transform:
                        value = value_transform(value)
                    user_totals[user_id] += value

            if not any(user_totals.values()):
                ax.set_title(f"{title} (no data)", fontname="Helvetica", color="#545454")
                ax.axis('off')
                return

            top_users = [u for u, _ in sorted(user_totals.items(), key=lambda x: x[1], reverse=True)[:8]]
            series_map = {u: [] for u in top_users}
            other_series = []
            for snap in user_snapshots:
                other_total = 0
                for user_id in top_users:
                    value = snap.get(user_id, {}).get(metric_key, 0)
                    if value_transform:
                        value = value_transform(value)
                    series_map[user_id].append(value)
                for user_id, vals in snap.items():
                    if user_id not in top_users:
                        value = vals.get(metric_key, 0)
                        if value_transform:
                            value = value_transform(value)
                        other_total += value
                other_series.append(other_total)

            labels = [self.get_known_user_name(u, prefer_name=True) for u in top_users]
            stacks = [series_map[u] for u in top_users]
            if any(other_series):
                labels.append("Other users")
                stacks.append(other_series)

            colors = [color_map.get(u, "#7f7f7f") for u in top_users]
            if any(other_series):
                colors.append("#7f7f7f")
            ax.stackplot(times, stacks, labels=labels, colors=colors, alpha=0.8)
            ax.set_title(title, fontname="Helvetica", color="#545454")
            legend = ax.legend(loc="upper left", ncol=2, fontsize=7)
            for text in legend.get_texts():
                text.set_color("#767676")
            ax.grid(True, alpha=0.3)
            ax.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d\n%H:%M"))
            ax.margins(x=0)

        fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(9, 10), sharex=True, constrained_layout=False)
        fig.subplots_adjust(hspace=0.20)
        for ax in axes:
            ax.set_facecolor("#EFEFEF")
            for spine in ax.spines.values():
                spine.set_color("#989898")
                spine.set_linewidth(0.4)
            ax.tick_params(colors="#989898", labelsize=7)
            ax.xaxis.label.set_color("#989898")
            ax.yaxis.label.set_color("#989898")

        ax_nodes = axes[0]
        if not nodes:
            ax_nodes.set_title("Active nodes", fontname="Helvetica", color="#545454")
            ax_nodes.axis('off')
        else:
            ax_nodes.fill_between(times, nodes, color="#2ca02c", alpha=0.25)
            ax_nodes.plot(times, nodes, color="#2ca02c", linewidth=0.5)
            ax_nodes.set_title("Num active nodes", fontname="Helvetica", color="#545454")
            ax_nodes.grid(True, alpha=0.3)
            ax_nodes.xaxis.set_major_formatter(mdates.DateFormatter("%m-%d\n%H:%M"))
            ax_nodes.margins(x=0)

        plot_user_stack(axes[1], "cpu", "CPU usage")
        plot_user_stack(axes[2], "mem_mb", "Memory Allocated (GB)", value_transform=lambda v: v / 1024)
        for ax in axes:
            ax.set_xlim(times[0], times[-1])

        if output_path:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            png_path = str(output_path)
        else:
            fd, png_path = tempfile.mkstemp(prefix="rosalind_usage_", suffix=".png")

        plt.savefig(png_path, dpi=150)
        plt.close(fig)
        return png_path


    def __test_run(self):
        for slurm_output_path in glob.glob('sandbox/slurm-outputs-for-testing/*.txt'):
            print('#' * (len(slurm_output_path) + 8))
            print(f'### {slurm_output_path} ###')
            print('#' * (len(slurm_output_path) + 8))
            print()

            slurm_queue = open(slurm_output_path).read()
            job_data = self.get_slurm_job_data(slurm_queue)

            self.check_user_changes(job_data)
            self.summarize_jobs(job_data)
            self.log_usage(job_data)

            time.sleep(1)


    def __actual_run(self):
        job_data = self.get_slurm_job_data()
        self.previous_users = self.get_current_users(job_data)

        if self.overall_summary_at_start:
            self.summarize_jobs(job_data)

        while True:
            loop_start = int(time.time())
            job_data = self.get_slurm_job_data()
            self.log_usage(job_data)

            if loop_start % self.user_jobs_interval < self.check_interval:
                # runs every minute (or custom interval)
                self.check_user_changes(job_data)

            if loop_start % self.overall_summary_interval < self.check_interval:
                # runs every summary interval
                self.summarize_jobs(job_data)

            time.sleep(self.check_interval)


    def run(self):
        """Main loop to check the Slurm queue and send updates to Slack."""

        if self.testing:
            self.__test_run()
        else:
            self.__actual_run()

    @staticmethod
    def _load_config(config_file=None):
        """Load YAML config that holds environment-specific settings."""

        try:
            import yaml
        except ImportError as e:  # pragma: no cover - dependency check
            raise RuntimeError("PyYAML is required to load configuration. Install it with `pip install pyyaml`.") from e

        config_path = Path(config_file) if config_file else Path(__file__).with_name(DEFAULT_CONFIG_FILENAME)

        if not config_path.is_file():
            template_path = config_path.with_suffix(config_path.suffix + ".template")
            hint = f" Copy the template with `cp {template_path} {config_path}` and fill in your secrets." if template_path.is_file() else ""
            raise RuntimeError(f"Missing config file at '{config_path}'. Provide one via --config-file.{hint}")

        with config_path.open() as handle:
            return yaml.safe_load(handle) or {}


def get_user_args():
    """Parses commandline arguments"""

    parser = argparse.ArgumentParser()
    groupA = parser.add_argument_group('ESSENTIALS', 'Essential information related to Slurm and Slack')
    groupA.add_argument('--group-id', metavar="ID", default=None, help="Your group ID or partition on Slurm. If you "
        "don't have one, you can leave this empty and see what happens (we haven't really tested it lol).")
    groupA.add_argument('--config-file', metavar="PATH", default=None, help=f"Path to the YAML config file. "
        f"Defaults to {DEFAULT_CONFIG_FILENAME} next to this script.")

    groupB = parser.add_argument_group('INTERVALS', 'Defaults are just fine, but you can also set anyting you want here')
    groupB.add_argument('--squeue-check-interval', metavar="SECONDS", default=60, type=int, help="The frequency of "
        "squeue output collection. The default of 60, which means rosalind will check the slurm output once every minute. "
        "It is a very bad idea to change this :/")
    groupB.add_argument('--overall-summary-interval', metavar="SECONDS", default=21600, type=int, help="The frequency of "
        "overall job summaries to be prepared and sent to Slack. The default is 21600, i.e., every 6 hour cycle.")
    groupB.add_argument('--user-jobs-interval', metavar="SECONDS", default=60, type=int, help="The frequency of "
        "checking changes in user jobs (new jobs, finished jobs, etc). The default is 60, i.e., every minute.")
    groupB.add_argument('--overall-summary-at-start', default=False, action="store_true", help="By default, the overal summary of jobs "
        "takes place every `--overall-summar-interval` around the clock. This flag ensures that at the beginning of "
        "the program there will be an overall summary just once regardless of the interval.")

    groupC = parser.add_argument_group('DEVELOPMENT & TESTING', 'Parameters to test the program.')
    groupC.add_argument('--stdout-only', action="store_true", default=False, help="Don't send anything to Slack, "
        "print messages that were meant to be sent to slack to the terminal.")
    groupC.add_argument('--testing', action="store_true", default=False, help="Declaring this flag will instruct the "
        "program to 'simulate' a bunch of slurm outputs, and will set all the intervals the way it sees fit. The "
        "messages that were meant to be sent to the Slack environment will also not be sent to the slack environment, "
        "but printed on screen just so you can see things.")

    return parser.parse_args()



if __name__ == "__main__":
    monitor = SlurmMonitor(get_user_args())
    monitor.run()
